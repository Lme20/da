{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"terminology/","text":"GLOSSARY Address A location in memory. Adjacency List An implementation for a graph that uses an (array-based) list to represent the vertices of the graph, and each vertex is in turn represented by a (linked) list of the vertices that are neighbors. Adjacency Matrix An implementation for a graph that uses a 2-dimensional array where each row and each column corresponds to a vertex in the graph. A given row and column in the matrix corresponds to an edge from the vertex corresponding to the row to the vertex corresponding to the column. Adjacent Two nodes of a tree or two vertices of a graph are said to be adjacent if they have an edge connecting them. If the edge is directed from a to b, then we say that a is adjacent to b, and b is adjacent from a. ADT Abbreviation for abstract data type. Adversary A fictional construct introduced for use in an adversary argument. Aggregate Type A data type whose members have subparts. For example, a typical database record. Another term for this is composite type. Amortized Complexity A modification to the notion of complexity for operations on a data structure where, for each fixed input size, one does not just look at the cost of a single run of the operation, but its amortized cost over sufficiently long series of operations of the same kind. This can be made precise without considering averages by introducing potentials. Amortized Cost The average cost of an operation in a sufficiently long series of operations of the same kind. This is as opposed to considering every individual operation to independently have its own cost, which might lead to an overestimate for the total cost of the series. This can be made precise without considering averages by introducing potentials. In amortized analysis, gives rise to the notion of amortized complexity. Asymptotic Complexity The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: time complexity (default) space complexity complexity in some other cost worst case (default) average case best case Term Whether to use amortized complexity. Average Case In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size n. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size n has its own average case. We never consider the average case as removed from input size. Basic operation A basic operation must have the property that its time to complete does not depend on the particular values of its operands. Adding or comparing two integer variables are examples of basic operations in most programming languages. Call Stack Known also as execution stack. A stack that stores the function call sequence and the return address for each function. Size Size is often the number of inputs processed. For example, when comparing sorting algorithms the size of the problem is typically measured by the number of records to be sorted. Height The number of nodes on the longest path from the root to the leaf.","title":"Terminology"},{"location":"terminology/#glossary","text":"","title":"GLOSSARY"},{"location":"terminology/#address","text":"A location in memory.","title":"Address"},{"location":"terminology/#adjacency-list","text":"An implementation for a graph that uses an (array-based) list to represent the vertices of the graph, and each vertex is in turn represented by a (linked) list of the vertices that are neighbors.","title":"Adjacency List"},{"location":"terminology/#adjacency-matrix","text":"An implementation for a graph that uses a 2-dimensional array where each row and each column corresponds to a vertex in the graph. A given row and column in the matrix corresponds to an edge from the vertex corresponding to the row to the vertex corresponding to the column.","title":"Adjacency Matrix"},{"location":"terminology/#adjacent","text":"Two nodes of a tree or two vertices of a graph are said to be adjacent if they have an edge connecting them. If the edge is directed from a to b, then we say that a is adjacent to b, and b is adjacent from a.","title":"Adjacent"},{"location":"terminology/#adt","text":"Abbreviation for abstract data type.","title":"ADT"},{"location":"terminology/#adversary","text":"A fictional construct introduced for use in an adversary argument.","title":"Adversary"},{"location":"terminology/#aggregate-type","text":"A data type whose members have subparts. For example, a typical database record. Another term for this is composite type.","title":"Aggregate Type"},{"location":"terminology/#amortized-complexity","text":"A modification to the notion of complexity for operations on a data structure where, for each fixed input size, one does not just look at the cost of a single run of the operation, but its amortized cost over sufficiently long series of operations of the same kind. This can be made precise without considering averages by introducing potentials.","title":"Amortized Complexity"},{"location":"terminology/#amortized-cost","text":"The average cost of an operation in a sufficiently long series of operations of the same kind. This is as opposed to considering every individual operation to independently have its own cost, which might lead to an overestimate for the total cost of the series. This can be made precise without considering averages by introducing potentials. In amortized analysis, gives rise to the notion of amortized complexity.","title":"Amortized Cost"},{"location":"terminology/#asymptotic-complexity","text":"The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: time complexity (default) space complexity complexity in some other cost worst case (default) average case best case","title":"Asymptotic Complexity"},{"location":"terminology/#term","text":"Whether to use amortized complexity.","title":"Term"},{"location":"terminology/#average-case","text":"In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size n. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size n has its own average case. We never consider the average case as removed from input size.","title":"Average Case"},{"location":"terminology/#basic-operation","text":"A basic operation must have the property that its time to complete does not depend on the particular values of its operands. Adding or comparing two integer variables are examples of basic operations in most programming languages.","title":"Basic operation"},{"location":"terminology/#call-stack","text":"Known also as execution stack. A stack that stores the function call sequence and the return address for each function.","title":"Call Stack"},{"location":"terminology/#size","text":"Size is often the number of inputs processed. For example, when comparing sorting algorithms the size of the problem is typically measured by the number of records to be sorted.","title":"Size"},{"location":"terminology/#height","text":"The number of nodes on the longest path from the root to the leaf.","title":"Height"},{"location":"abstract-data-types/ADT/","text":"ABSTRACT-DATA-TYPES Graph What is it? A collection of nodes (vertices) and edges connecting pairs of nodes. What does it do? Represents relationships or connections between entities. Usage Social Networks : Representing friendships where each person is a node and an edge represents a friendship. Maps : Representing cities (nodes) and roads (edges) connecting them. Map What is it? A collection of key-value pairs where each key is unique. What does it do? Allows for fast retrieval, insertion, and deletion of values based on keys. Usage Phone Book : Names as keys and phone numbers as values. Dictionary : Words as keys and definitions as values. Priority Queue What is it? A data structure where each element has a priority and the element with the highest priority is served first. What does it do? Manages a set of records with dynamically changing priorities. Usage Task Scheduling : Operating systems managing processes where some tasks have higher priority. Dijkstra\u2019s Algorithm : Finding the shortest path in a graph where the next node to explore is the one with the smallest tentative distance. Queue What is it? A collection of elements that supports adding elements to the rear and removing elements from the front (FIFO: First-In-First-Out). What does it do? Manages ordered sequences of items. Usage Customer Service : Customers lining up in the order they arrive. Print Queue : Print jobs processed in the order they are received. Set What is it? A collection of unique elements. What does it do? Supports operations like union, intersection, and difference. Usage Unique Items : Tracking unique visitors to a website. Tagging : Managing tags where each tag is unique. Stack What is it? A collection of elements that supports adding and removing elements from the top (LIFO: Last-In-First-Out). What does it do? Manages elements in a reverse order. Usage Undo Mechanism : In text editors where the last operation is undone first. Function Call Management : Call stack in programming where the last called function is executed first.","title":"ADT"},{"location":"abstract-data-types/ADT/#abstract-data-types","text":"","title":"ABSTRACT-DATA-TYPES"},{"location":"abstract-data-types/ADT/#graph","text":"","title":"Graph"},{"location":"abstract-data-types/ADT/#what-is-it","text":"A collection of nodes (vertices) and edges connecting pairs of nodes.","title":"What is it?"},{"location":"abstract-data-types/ADT/#what-does-it-do","text":"Represents relationships or connections between entities.","title":"What does it do?"},{"location":"abstract-data-types/ADT/#usage","text":"Social Networks : Representing friendships where each person is a node and an edge represents a friendship. Maps : Representing cities (nodes) and roads (edges) connecting them.","title":"Usage"},{"location":"abstract-data-types/ADT/#map","text":"","title":"Map"},{"location":"abstract-data-types/ADT/#what-is-it_1","text":"A collection of key-value pairs where each key is unique.","title":"What is it?"},{"location":"abstract-data-types/ADT/#what-does-it-do_1","text":"Allows for fast retrieval, insertion, and deletion of values based on keys.","title":"What does it do?"},{"location":"abstract-data-types/ADT/#usage_1","text":"Phone Book : Names as keys and phone numbers as values. Dictionary : Words as keys and definitions as values.","title":"Usage"},{"location":"abstract-data-types/ADT/#priority-queue","text":"","title":"Priority Queue"},{"location":"abstract-data-types/ADT/#what-is-it_2","text":"A data structure where each element has a priority and the element with the highest priority is served first.","title":"What is it?"},{"location":"abstract-data-types/ADT/#what-does-it-do_2","text":"Manages a set of records with dynamically changing priorities.","title":"What does it do?"},{"location":"abstract-data-types/ADT/#usage_2","text":"Task Scheduling : Operating systems managing processes where some tasks have higher priority. Dijkstra\u2019s Algorithm : Finding the shortest path in a graph where the next node to explore is the one with the smallest tentative distance.","title":"Usage"},{"location":"abstract-data-types/ADT/#queue","text":"","title":"Queue"},{"location":"abstract-data-types/ADT/#what-is-it_3","text":"A collection of elements that supports adding elements to the rear and removing elements from the front (FIFO: First-In-First-Out).","title":"What is it?"},{"location":"abstract-data-types/ADT/#what-does-it-do_3","text":"Manages ordered sequences of items.","title":"What does it do?"},{"location":"abstract-data-types/ADT/#usage_3","text":"Customer Service : Customers lining up in the order they arrive. Print Queue : Print jobs processed in the order they are received.","title":"Usage"},{"location":"abstract-data-types/ADT/#set","text":"","title":"Set"},{"location":"abstract-data-types/ADT/#what-is-it_4","text":"A collection of unique elements.","title":"What is it?"},{"location":"abstract-data-types/ADT/#what-does-it-do_4","text":"Supports operations like union, intersection, and difference.","title":"What does it do?"},{"location":"abstract-data-types/ADT/#usage_4","text":"Unique Items : Tracking unique visitors to a website. Tagging : Managing tags where each tag is unique.","title":"Usage"},{"location":"abstract-data-types/ADT/#stack","text":"","title":"Stack"},{"location":"abstract-data-types/ADT/#what-is-it_5","text":"A collection of elements that supports adding and removing elements from the top (LIFO: Last-In-First-Out).","title":"What is it?"},{"location":"abstract-data-types/ADT/#what-does-it-do_5","text":"Manages elements in a reverse order.","title":"What does it do?"},{"location":"abstract-data-types/ADT/#usage_5","text":"Undo Mechanism : In text editors where the last operation is undone first. Function Call Management : Call stack in programming where the last called function is executed first.","title":"Usage"},{"location":"abstract-data-types/graph/","text":"GRAPH","title":"Graph"},{"location":"abstract-data-types/graph/#graph","text":"","title":"GRAPH"},{"location":"abstract-data-types/list/","text":"LIST","title":"List"},{"location":"abstract-data-types/list/#list","text":"","title":"LIST"},{"location":"abstract-data-types/map/","text":"MAP","title":"Map"},{"location":"abstract-data-types/map/#map","text":"","title":"MAP"},{"location":"abstract-data-types/priority-queues/","text":"Priority Queues A priority queue is an ADTs. Similar to queues, it operates by: Adding elements with a priority: Each element in the priority queue is associated with a priority. The priority can be any comparable value such as an integer or float. Removing elements with highest priority first: When removing elements from a priority queue, the element with the highest prioriy gets removed first. If two elements have the same priority, the order of removal can be based on their order of insertion or other criterion. Only supports elements that are comparable Very common in Dijkstra's Shortest Path Algorithm and MST (Prim's, Kruskal's) FIFO ADT (First-in First Out) Implementation 1. Binary Heaps 2. Balanced Binary Search Tree 3. Unsorted List 4. Sorted List Complexity (with Binary Heaps) Binary Heap construction: O(N) Polling: O(log N) Peeking: O(1) Adding: O(log N) Naive removing: O(N) Advanced removing (with hash table): O(log N) Naive contains: O(N) Contains check (with hash table): O(1)","title":"Priority Queues"},{"location":"abstract-data-types/priority-queues/#priority-queues","text":"A priority queue is an ADTs. Similar to queues, it operates by: Adding elements with a priority: Each element in the priority queue is associated with a priority. The priority can be any comparable value such as an integer or float. Removing elements with highest priority first: When removing elements from a priority queue, the element with the highest prioriy gets removed first. If two elements have the same priority, the order of removal can be based on their order of insertion or other criterion. Only supports elements that are comparable Very common in Dijkstra's Shortest Path Algorithm and MST (Prim's, Kruskal's) FIFO ADT (First-in First Out)","title":"Priority Queues"},{"location":"abstract-data-types/priority-queues/#implementation","text":"","title":"Implementation"},{"location":"abstract-data-types/priority-queues/#1-binary-heaps","text":"","title":"1. Binary Heaps"},{"location":"abstract-data-types/priority-queues/#2-balanced-binary-search-tree","text":"","title":"2. Balanced Binary Search Tree"},{"location":"abstract-data-types/priority-queues/#3-unsorted-list","text":"","title":"3. Unsorted List"},{"location":"abstract-data-types/priority-queues/#4-sorted-list","text":"","title":"4. Sorted List"},{"location":"abstract-data-types/priority-queues/#complexity-with-binary-heaps","text":"Binary Heap construction: O(N) Polling: O(log N) Peeking: O(1) Adding: O(log N) Naive removing: O(N) Advanced removing (with hash table): O(log N) Naive contains: O(N) Contains check (with hash table): O(1)","title":"Complexity (with Binary Heaps)"},{"location":"abstract-data-types/queue/","text":"QUEUES","title":"Queue"},{"location":"abstract-data-types/queue/#queues","text":"","title":"QUEUES"},{"location":"abstract-data-types/set/","text":"SETS","title":"Set"},{"location":"abstract-data-types/set/#sets","text":"","title":"SETS"},{"location":"abstract-data-types/stack/","text":"STACKS","title":"Stack"},{"location":"abstract-data-types/stack/#stacks","text":"","title":"STACKS"},{"location":"algorithm-analysis/algorithm_analysis/","text":"Algorithm analysis Problems, Algorithms and Programs Problems A problem maps inputs to outputs. A problem is a mapping from inputs to outputs, and there might be many algorithms that can accomplish this mapping. Programs A program is an instantiation of an algorithm implemented in a specific programming language. Algorithms An algorithm is a series of steps that act as a recipe to solve a particular problem. Comparing algorithms To estimate an algorithm's performance: * count the number of basic operations Complexity Worst-Case Average-case Best-case Order of Growth - Growth Rate Big-O Notation 1. RULES Constant time operations O(1) basic arithmetic operations (e.g., a + b) assigning values (e.g., x = 10) accessing elements by index (e.g., array[i]) Example: sum = a + b, array[i] = 5. Linear time operations O(N) - Operations that iterate over a lits of elements or list once Example: For-loop iterating over N elements QUADRATIC TIME OPERATIONS O(N^2): Nested for-loops where each loop runs 'n' times Example: Bubble sort, insertion sort, selection sort CUBIC/POLYNOMIAL TIME OPERATIONS O(n^3)/O(n^k): 3 or more nested for-loops Logarithmic time operations O(log N) - Divide and conquer algorithms, binary search, operations in balanced binary search trees (e.g. AVL, red-black trees) Example: Binary search in a sorted array Linearithmic time operations O(N log N) - Sorting algorithms like heapsort, mergesort Example: sorting an array using merge sort Exponential time operations O(2^N) - Algorithms where all subsets or combinations are explored Example: Recursive algorithms Factorial time operations O(N!) - Algorithms that generate permutations of a set Example: Brute-force algorithms Applied rates of growth (Heuristics) Single loops: usually results in O(N) Nested loops: Where each loop runs '\ud835\udc5b' times results in O(n2), O(n3), etc (count the number of nested loops) Logarithmic loops: Where a problem size is divided in each iteration results in O(log N) while n > 1: n = n / 2 # O(1) operation Different loop bounds: Their complexities should be added, not for nested loops for i in range(n): # O(n) # O(1) operation for j in range(m): # O(m) # O(1) operation # Total: O(n) + O(m) Non-uniform operations: Operations with different complexities based on certain conditions. Usually, you should only consider the worst-case. for i in range(n): if i % 2 == 0: # O(1) operation else: # O(log n) operation # Total: O(n * log n) because we consider the worst-case Conditionals (if-else statements): Depends on the complexity of the operations within them, Usually, you should only consider the branch with the worst-case. if condition: # O(n) operation else: # O(log n) operation # Total: O(n) because it's the dominant term Divide-and-conquer: usually algorithms with O(log N) or O(N log N) complexities Operations on Data structures: * Array/list access: O(1) for accessing elements in index * Linked list operations: O(N) for search, O(1) for insertion/deletion * BST: O(log N) for all BST operations * Hash tables: O(1) for all operations, O(N) for worst-case Combining complexities: independent operations: add their complexities ``` for i in range(n): # O(n) # O(1) operation for j in range(n): # O(n) # O(1) operation Total: O(n) + O(n) = O(n) nested operations: multiply their complexities ``` for i in range(n): # O(n) for j in range(n): # O(n) # O(1) operation Total: O(n) * O(n) = O(n^2) Asymptotic Complexity Algorithmic behaviours and time complexity 1. Constant Time: O(1) Behavior : The execution time does not depend on the size of the input data. The operations are fixed and do not change as the input size changes. Examples : Accessing an array element by index, checking if a stack is empty, or assigning a value to a variable. 2. Logarithmic Time: O(log n) Behavior : The execution time increases logarithmically as the input size increases. This usually occurs in algorithms that divide the problem in half each step or work on reducing the problem size exponentially. Examples : Binary search in a sorted array, finding an element in a balanced binary search tree. 3. Linear Time: O(n) Behavior : The execution time increases linearly with the increase in input size. These algorithms typically perform a single operation on each element of the input. Examples : Summing all elements in an array, checking for the existence of an element in an unsorted list. 4. Linearithmic Time: O(n log n) Behavior : The execution combines linear and logarithmic behaviors. These are often efficient sorting algorithms that recursively divide and conquer the input. Examples : Merge sort, quicksort (on average). 5. Quadratic Time: O(n\u00b2) Behavior : The execution time grows as the square of the input size. These algorithms usually involve nested iterations over the data set. Examples : Bubble sort, insertion sort, checking all pairs in an array (like finding duplicates). 6. Cubic Time: O(n\u00b3) Behavior : The execution time is proportional to the cube of the input size. This is typical in algorithms involving three nested loops. Examples : Naive algorithms for matrix multiplication, solving three-body problems in physics. 7. Exponential Time: O(2^n) Behavior : The execution time doubles with each addition to the input size. These algorithms often deal with all possible combinations of inputs. Examples : Brute-force solutions for the traveling salesman problem, generating all subsets of a set. 8. Factorial Time: O(n!) Behavior : The execution time grows factorial with the input size. These are algorithms that generate all possible permutations of the input. Examples : Solving the traveling salesman problem by trying every permutation of visits.","title":"Algorithm Analysis"},{"location":"algorithm-analysis/algorithm_analysis/#algorithm-analysis","text":"","title":"Algorithm analysis"},{"location":"algorithm-analysis/algorithm_analysis/#problems-algorithms-and-programs","text":"","title":"Problems, Algorithms and Programs"},{"location":"algorithm-analysis/algorithm_analysis/#problems","text":"A problem maps inputs to outputs. A problem is a mapping from inputs to outputs, and there might be many algorithms that can accomplish this mapping.","title":"Problems"},{"location":"algorithm-analysis/algorithm_analysis/#programs","text":"A program is an instantiation of an algorithm implemented in a specific programming language.","title":"Programs"},{"location":"algorithm-analysis/algorithm_analysis/#algorithms","text":"An algorithm is a series of steps that act as a recipe to solve a particular problem.","title":"Algorithms"},{"location":"algorithm-analysis/algorithm_analysis/#comparing-algorithms","text":"To estimate an algorithm's performance: * count the number of basic operations","title":"Comparing algorithms"},{"location":"algorithm-analysis/algorithm_analysis/#complexity","text":"","title":"Complexity"},{"location":"algorithm-analysis/algorithm_analysis/#worst-case","text":"","title":"Worst-Case"},{"location":"algorithm-analysis/algorithm_analysis/#average-case","text":"","title":"Average-case"},{"location":"algorithm-analysis/algorithm_analysis/#best-case","text":"","title":"Best-case"},{"location":"algorithm-analysis/algorithm_analysis/#order-of-growth-growth-rate","text":"","title":"Order of Growth - Growth Rate"},{"location":"algorithm-analysis/algorithm_analysis/#big-o-notation","text":"1. RULES Constant time operations O(1) basic arithmetic operations (e.g., a + b) assigning values (e.g., x = 10) accessing elements by index (e.g., array[i]) Example: sum = a + b, array[i] = 5. Linear time operations O(N) - Operations that iterate over a lits of elements or list once Example: For-loop iterating over N elements QUADRATIC TIME OPERATIONS O(N^2): Nested for-loops where each loop runs 'n' times Example: Bubble sort, insertion sort, selection sort CUBIC/POLYNOMIAL TIME OPERATIONS O(n^3)/O(n^k): 3 or more nested for-loops Logarithmic time operations O(log N) - Divide and conquer algorithms, binary search, operations in balanced binary search trees (e.g. AVL, red-black trees) Example: Binary search in a sorted array Linearithmic time operations O(N log N) - Sorting algorithms like heapsort, mergesort Example: sorting an array using merge sort Exponential time operations O(2^N) - Algorithms where all subsets or combinations are explored Example: Recursive algorithms Factorial time operations O(N!) - Algorithms that generate permutations of a set Example: Brute-force algorithms Applied rates of growth (Heuristics) Single loops: usually results in O(N) Nested loops: Where each loop runs '\ud835\udc5b' times results in O(n2), O(n3), etc (count the number of nested loops) Logarithmic loops: Where a problem size is divided in each iteration results in O(log N) while n > 1: n = n / 2 # O(1) operation Different loop bounds: Their complexities should be added, not for nested loops for i in range(n): # O(n) # O(1) operation for j in range(m): # O(m) # O(1) operation # Total: O(n) + O(m) Non-uniform operations: Operations with different complexities based on certain conditions. Usually, you should only consider the worst-case. for i in range(n): if i % 2 == 0: # O(1) operation else: # O(log n) operation # Total: O(n * log n) because we consider the worst-case Conditionals (if-else statements): Depends on the complexity of the operations within them, Usually, you should only consider the branch with the worst-case. if condition: # O(n) operation else: # O(log n) operation # Total: O(n) because it's the dominant term Divide-and-conquer: usually algorithms with O(log N) or O(N log N) complexities Operations on Data structures: * Array/list access: O(1) for accessing elements in index * Linked list operations: O(N) for search, O(1) for insertion/deletion * BST: O(log N) for all BST operations * Hash tables: O(1) for all operations, O(N) for worst-case Combining complexities: independent operations: add their complexities ``` for i in range(n): # O(n) # O(1) operation for j in range(n): # O(n) # O(1) operation","title":"Big-O Notation"},{"location":"algorithm-analysis/algorithm_analysis/#total-on-on-on","text":"nested operations: multiply their complexities ``` for i in range(n): # O(n) for j in range(n): # O(n) # O(1) operation","title":"Total: O(n) + O(n) = O(n)"},{"location":"algorithm-analysis/algorithm_analysis/#total-on-on-on2","text":"","title":"Total: O(n) * O(n) = O(n^2)"},{"location":"algorithm-analysis/algorithm_analysis/#asymptotic-complexity","text":"","title":"Asymptotic Complexity"},{"location":"algorithm-analysis/algorithm_analysis/#algorithmic-behaviours-and-time-complexity","text":"","title":"Algorithmic behaviours and time complexity"},{"location":"algorithm-analysis/algorithm_analysis/#1-constant-time-o1","text":"Behavior : The execution time does not depend on the size of the input data. The operations are fixed and do not change as the input size changes. Examples : Accessing an array element by index, checking if a stack is empty, or assigning a value to a variable.","title":"1. Constant Time: O(1)"},{"location":"algorithm-analysis/algorithm_analysis/#2-logarithmic-time-olog-n","text":"Behavior : The execution time increases logarithmically as the input size increases. This usually occurs in algorithms that divide the problem in half each step or work on reducing the problem size exponentially. Examples : Binary search in a sorted array, finding an element in a balanced binary search tree.","title":"2. Logarithmic Time: O(log n)"},{"location":"algorithm-analysis/algorithm_analysis/#3-linear-time-on","text":"Behavior : The execution time increases linearly with the increase in input size. These algorithms typically perform a single operation on each element of the input. Examples : Summing all elements in an array, checking for the existence of an element in an unsorted list.","title":"3. Linear Time: O(n)"},{"location":"algorithm-analysis/algorithm_analysis/#4-linearithmic-time-on-log-n","text":"Behavior : The execution combines linear and logarithmic behaviors. These are often efficient sorting algorithms that recursively divide and conquer the input. Examples : Merge sort, quicksort (on average).","title":"4. Linearithmic Time: O(n log n)"},{"location":"algorithm-analysis/algorithm_analysis/#5-quadratic-time-on2","text":"Behavior : The execution time grows as the square of the input size. These algorithms usually involve nested iterations over the data set. Examples : Bubble sort, insertion sort, checking all pairs in an array (like finding duplicates).","title":"5. Quadratic Time: O(n\u00b2)"},{"location":"algorithm-analysis/algorithm_analysis/#6-cubic-time-on3","text":"Behavior : The execution time is proportional to the cube of the input size. This is typical in algorithms involving three nested loops. Examples : Naive algorithms for matrix multiplication, solving three-body problems in physics.","title":"6. Cubic Time: O(n\u00b3)"},{"location":"algorithm-analysis/algorithm_analysis/#7-exponential-time-o2n","text":"Behavior : The execution time doubles with each addition to the input size. These algorithms often deal with all possible combinations of inputs. Examples : Brute-force solutions for the traveling salesman problem, generating all subsets of a set.","title":"7. Exponential Time: O(2^n)"},{"location":"algorithm-analysis/algorithm_analysis/#8-factorial-time-on","text":"Behavior : The execution time grows factorial with the input size. These are algorithms that generate all possible permutations of the input. Examples : Solving the traveling salesman problem by trying every permutation of visits.","title":"8. Factorial Time: O(n!)"},{"location":"algorithms/dynamic/optimal-bst/","text":"","title":"Optimal BST"},{"location":"algorithms/graph-traversal/bfs/breadth-first-search/","text":"Breadth-first search (BFS) - Find the shortest path Examines all vertices connected to the start vertex (node), used to explore nodes and edges of a graph Has a queue stack instead of a recursion stack Breadth-first search (BFS) is commonly used for unweighted graphs BFS is very useful to find the shortest path in an unweighted graph BFS can't be used for finding the shortest path in a weighted graph, use Dijkstras instead Essentially, contrary to DFS, BFS explores nodes in \"layers\" Uses a queue data structure Steps Start at the root node (or any specified starting node). Add neighboring nodes (nodes attached to root node) to queue if no more unvisited neighbors, go to next neighboring node (based on queue order) Continue step 2 with neighboring node Conclude BFS when all nodes have been visited UNDIRECTED AND DIRECTED BFS Pseudocode #Global/ class scope variables n = number of nodes in the graph g = adjacency list representing unweighted graph #s = start node, e= end node, and 0 \u2264 e, s < n function bfs(s, e): #Do a BFS starting at node s prev = solve(s) #Return reconstructed path from s->e return reconstructPath(s, e, prev) Time Complexity - Operations total complexity: O(V+E)","title":"BFS"},{"location":"algorithms/graph-traversal/bfs/breadth-first-search/#breadth-first-search-bfs-find-the-shortest-path","text":"Examines all vertices connected to the start vertex (node), used to explore nodes and edges of a graph Has a queue stack instead of a recursion stack Breadth-first search (BFS) is commonly used for unweighted graphs BFS is very useful to find the shortest path in an unweighted graph BFS can't be used for finding the shortest path in a weighted graph, use Dijkstras instead Essentially, contrary to DFS, BFS explores nodes in \"layers\" Uses a queue data structure","title":"Breadth-first search (BFS) - Find the shortest path"},{"location":"algorithms/graph-traversal/bfs/breadth-first-search/#steps","text":"Start at the root node (or any specified starting node). Add neighboring nodes (nodes attached to root node) to queue if no more unvisited neighbors, go to next neighboring node (based on queue order) Continue step 2 with neighboring node Conclude BFS when all nodes have been visited","title":"Steps"},{"location":"algorithms/graph-traversal/bfs/breadth-first-search/#undirected-and-directed-bfs","text":"","title":"UNDIRECTED AND DIRECTED BFS"},{"location":"algorithms/graph-traversal/bfs/breadth-first-search/#pseudocode","text":"#Global/ class scope variables n = number of nodes in the graph g = adjacency list representing unweighted graph #s = start node, e= end node, and 0 \u2264 e, s < n function bfs(s, e): #Do a BFS starting at node s prev = solve(s) #Return reconstructed path from s->e return reconstructPath(s, e, prev)","title":"Pseudocode"},{"location":"algorithms/graph-traversal/bfs/breadth-first-search/#time-complexity-operations","text":"total complexity: O(V+E)","title":"Time Complexity - Operations"},{"location":"algorithms/graph-traversal/dfs/depth-first-search/","text":"DEPTH-FIRST-SEARCH - explore nodes and edges of a graph Not very useful used by itself Better used when augmented for counting connected components, determine connectivity, find bridges points, etc Plunges into a graph without regard for which edge it takes next Continues until no new edge can be attained, where it backtracks and continues Finding connected components: separated sections of a graph - Assign integer values to each group to tell them apart - Start DFS at every node (except if already visited) - mark all reachable nodes with the integer value of its corresponding group What can DFS do? - Compute a graph's minimum-spanning tree - Detect and find cycles in a graph - Check if a graph is bipartite - Find strongly connected components - Topologically sort nodes of a graph - Find bridges and articulation points - Find articulating paths in a flow network - Generate mazes DIRECTED DEPTH-FIRST-SEARCH Follows only the direction of edges (from a source node to a destination node). processes each edge once UNDIRECTED DEPTH-FIRST-SEARCH Can traverse back and forth along any edge. processes each edge from both directions STEPS Start at node 0 (can start at any node) Pick any available node from node 0 (follow the alphabetical/numerical order if any) If node has been visited, backtrack (go back to a previously visited node where new unvisited nodes can be visited) If node is new, continue Continue until reaching node with dead end, if dead end, backtrack DFS ends when all reachable nodes from the initial node have been visited. OPERATIONS - COMPLEXITY Initialization: O(1) Traversal: O(V+E), where V is the number of vertices and E is the number of edges. Marking nodes: O(1) Processing nodes: O(1) Backtracking: O(1) Overall complexity O(V+E) PSEUDOCODE Does not include pseudocode for finding connected components. # Global or class scope variables n = number of nodes in the graph g = Adjacency list representing graph visited = [false,...,false] # size n function dfs(at): if visited[at] = return visited[at] = true neighbors = graph[at] for next in neighbors: dfs(next) #start dfs at node 0 start_node= 0 dfs(start_node)","title":"DFS"},{"location":"algorithms/graph-traversal/dfs/depth-first-search/#depth-first-search-explore-nodes-and-edges-of-a-graph","text":"Not very useful used by itself Better used when augmented for counting connected components, determine connectivity, find bridges points, etc Plunges into a graph without regard for which edge it takes next Continues until no new edge can be attained, where it backtracks and continues Finding connected components: separated sections of a graph - Assign integer values to each group to tell them apart - Start DFS at every node (except if already visited) - mark all reachable nodes with the integer value of its corresponding group What can DFS do? - Compute a graph's minimum-spanning tree - Detect and find cycles in a graph - Check if a graph is bipartite - Find strongly connected components - Topologically sort nodes of a graph - Find bridges and articulation points - Find articulating paths in a flow network - Generate mazes","title":"DEPTH-FIRST-SEARCH - explore nodes and edges of a graph"},{"location":"algorithms/graph-traversal/dfs/depth-first-search/#directed-depth-first-search","text":"Follows only the direction of edges (from a source node to a destination node). processes each edge once","title":"DIRECTED DEPTH-FIRST-SEARCH"},{"location":"algorithms/graph-traversal/dfs/depth-first-search/#undirected-depth-first-search","text":"Can traverse back and forth along any edge. processes each edge from both directions","title":"UNDIRECTED DEPTH-FIRST-SEARCH"},{"location":"algorithms/graph-traversal/dfs/depth-first-search/#steps","text":"Start at node 0 (can start at any node) Pick any available node from node 0 (follow the alphabetical/numerical order if any) If node has been visited, backtrack (go back to a previously visited node where new unvisited nodes can be visited) If node is new, continue Continue until reaching node with dead end, if dead end, backtrack DFS ends when all reachable nodes from the initial node have been visited.","title":"STEPS"},{"location":"algorithms/graph-traversal/dfs/depth-first-search/#operations-complexity","text":"Initialization: O(1) Traversal: O(V+E), where V is the number of vertices and E is the number of edges. Marking nodes: O(1) Processing nodes: O(1) Backtracking: O(1) Overall complexity O(V+E)","title":"OPERATIONS - COMPLEXITY"},{"location":"algorithms/graph-traversal/dfs/depth-first-search/#pseudocode","text":"Does not include pseudocode for finding connected components. # Global or class scope variables n = number of nodes in the graph g = Adjacency list representing graph visited = [false,...,false] # size n function dfs(at): if visited[at] = return visited[at] = true neighbors = graph[at] for next in neighbors: dfs(next) #start dfs at node 0 start_node= 0 dfs(start_node)","title":"PSEUDOCODE"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/","text":"Uniform-Cost search (UCS) - Find the shortest path UCS is a variant of Dijkstra's algorithm Steps Calculating values Pseudocode Implementation Time Complexity Space Complexity","title":"UCS"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/#uniform-cost-search-ucs-find-the-shortest-path","text":"UCS is a variant of Dijkstra's algorithm","title":"Uniform-Cost search (UCS) - Find the shortest path"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/#steps","text":"","title":"Steps"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/#calculating-values","text":"","title":"Calculating values"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/#pseudocode","text":"","title":"Pseudocode"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/#implementation","text":"","title":"Implementation"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/#time-complexity","text":"","title":"Time Complexity"},{"location":"algorithms/graph-traversal/ucs/uniform-cost-search/#space-complexity","text":"","title":"Space Complexity"},{"location":"algorithms/min-spanning-tree/kruskals/kruskals/","text":"Kruskal's algorithm - Minimum Spanning Trees Kruskal's a greedy algorithm MST: a subset of edges which connect all vertices in the graph with the minimal total edge cost. GOAL: To find the Minimum Spanning Tree (MST) of a connected, undirected graph. Cycle: formed when two nodes that are already connected through edges form a loop. Steps Sort all edges: Sort all edges of the graph by ascendig edge weight (smallest to largest) Process each edge: For each edge, process the 2 nodes the edge connects if nodes already unified (i.e., they belong to the same subset) = discard edge otherwise, include edge in the MST and unify the 2 subsets. Termination: Terminate algorithm when every edge has been processed or all nodes have been unified. That is, when [n - 1] edges are reached. NOTE: unified nodes should be ignored to avoid cycles. Calculating the total weight: - Simply add all of the weights found in every edge of the MST. Alternative steps: Select an edge of G of minimum weight Select a remaining edge of G of minimum weight Select any remaining edge of G that does not form a cycle with previously selected edges Repeat cycle 3 until n-1 edges have been reached Pseudocode Time Complexity Overall complexity: O(E log E) Average complexity: O(V log E) Where E is the number of edges in the graph. Space Complexity","title":"Kruskal's"},{"location":"algorithms/min-spanning-tree/kruskals/kruskals/#kruskals-algorithm-minimum-spanning-trees","text":"Kruskal's a greedy algorithm MST: a subset of edges which connect all vertices in the graph with the minimal total edge cost. GOAL: To find the Minimum Spanning Tree (MST) of a connected, undirected graph. Cycle: formed when two nodes that are already connected through edges form a loop.","title":"Kruskal's algorithm - Minimum Spanning Trees"},{"location":"algorithms/min-spanning-tree/kruskals/kruskals/#steps","text":"Sort all edges: Sort all edges of the graph by ascendig edge weight (smallest to largest) Process each edge: For each edge, process the 2 nodes the edge connects if nodes already unified (i.e., they belong to the same subset) = discard edge otherwise, include edge in the MST and unify the 2 subsets. Termination: Terminate algorithm when every edge has been processed or all nodes have been unified. That is, when [n - 1] edges are reached. NOTE: unified nodes should be ignored to avoid cycles. Calculating the total weight: - Simply add all of the weights found in every edge of the MST. Alternative steps: Select an edge of G of minimum weight Select a remaining edge of G of minimum weight Select any remaining edge of G that does not form a cycle with previously selected edges Repeat cycle 3 until n-1 edges have been reached","title":"Steps"},{"location":"algorithms/min-spanning-tree/kruskals/kruskals/#pseudocode","text":"","title":"Pseudocode"},{"location":"algorithms/min-spanning-tree/kruskals/kruskals/#time-complexity","text":"Overall complexity: O(E log E) Average complexity: O(V log E) Where E is the number of edges in the graph.","title":"Time Complexity"},{"location":"algorithms/min-spanning-tree/kruskals/kruskals/#space-complexity","text":"","title":"Space Complexity"},{"location":"algorithms/min-spanning-tree/prims/prims/","text":"Prim's algorithm - Minimal Cost Spanning Trees Results in a minimum-spanning tree, minimum weight connected graph, no cycles can Also be used with a priority queue GOAL: find the Minimum Spanning Tree (MST) of a graph. - Example of a greedy algorithm Steps Initialize the MST with a single vertex chosen arbitrarily. (pick a node on the graph) While the MST does not contain all vertices: Find the edge with the minimum weight that connects a vertex in the MST to a vertex outside the MST. Add this edge to the MST. Repeat until all vertices are included in the MST. Calculating values Start from an arbitrary vertex, add it to the MST. Use a priority queue to select the edge with the minimum weight that connects the MST to a new vertex. Continue selecting the smallest edge until all vertices are included. Pseudocode Implementation Time Complexity O(E log V) using an adjacency list and a priority queue. (binary heap) O(V^2) using an adjacency matrix. Space Complexity O(V + E) using an adjacency list. O(V^2) using an adjacency matrix.","title":"Prim's"},{"location":"algorithms/min-spanning-tree/prims/prims/#prims-algorithm-minimal-cost-spanning-trees","text":"Results in a minimum-spanning tree, minimum weight connected graph, no cycles can Also be used with a priority queue GOAL: find the Minimum Spanning Tree (MST) of a graph. - Example of a greedy algorithm","title":"Prim's algorithm - Minimal Cost Spanning Trees"},{"location":"algorithms/min-spanning-tree/prims/prims/#steps","text":"Initialize the MST with a single vertex chosen arbitrarily. (pick a node on the graph) While the MST does not contain all vertices: Find the edge with the minimum weight that connects a vertex in the MST to a vertex outside the MST. Add this edge to the MST. Repeat until all vertices are included in the MST.","title":"Steps"},{"location":"algorithms/min-spanning-tree/prims/prims/#calculating-values","text":"Start from an arbitrary vertex, add it to the MST. Use a priority queue to select the edge with the minimum weight that connects the MST to a new vertex. Continue selecting the smallest edge until all vertices are included.","title":"Calculating values"},{"location":"algorithms/min-spanning-tree/prims/prims/#pseudocode","text":"","title":"Pseudocode"},{"location":"algorithms/min-spanning-tree/prims/prims/#implementation","text":"","title":"Implementation"},{"location":"algorithms/min-spanning-tree/prims/prims/#time-complexity","text":"O(E log V) using an adjacency list and a priority queue. (binary heap) O(V^2) using an adjacency matrix.","title":"Time Complexity"},{"location":"algorithms/min-spanning-tree/prims/prims/#space-complexity","text":"O(V + E) using an adjacency list. O(V^2) using an adjacency matrix.","title":"Space Complexity"},{"location":"algorithms/searching/binary-search/binary-search/","text":"BINARY SEARCH - Find a target value in a sorted array Used to find a specific value in a sorted sequence Repeatedly divides the search space in half until the target value is found Works only on sorted arrays or sorted lists You should count the index not the value of the elements! random access of elements With each iteration, half of the values are eliminated Commonly used in arrays Steps Step 1: - Start with the entire sorted sequence. Step 2: - Calculate the middle index of the current search space to place the pivot. (not the elemenet value!) pivot = (left + right) / 2 Step 3: - Compare the middle element with the target value. If the middle element is equal to the target value, return its index. If the middle element is greater than the target value, repeat the process on the left half of the sequence. If the middle element is less than the target value, repeat the process on the right half of the sequence. NOTE: UPDATE middle element all the time! if search left then update left to mid - 1, if search right then update right to mid + 1 Step 4: - Continue dividing the search space in half until the target value is found or the search space is empty. Calculating values Middle index is calculated using the formula: mid = left + (right - left) / 2 , where left and right represent the indices of the current search space. Pseudocode Asymptotic complexity: Time complexity: best case: O(1) average case: O(log N) worst case: O(log N) O(log n) worst-case where n is the size of the sorted sequence. Space complexity: O(1) worst-case Only requires a constant amount of additional space for storing values.","title":"Binary Search"},{"location":"algorithms/searching/binary-search/binary-search/#binary-search-find-a-target-value-in-a-sorted-array","text":"Used to find a specific value in a sorted sequence Repeatedly divides the search space in half until the target value is found Works only on sorted arrays or sorted lists You should count the index not the value of the elements! random access of elements With each iteration, half of the values are eliminated Commonly used in arrays","title":"BINARY SEARCH - Find a target value in a sorted array"},{"location":"algorithms/searching/binary-search/binary-search/#steps","text":"Step 1: - Start with the entire sorted sequence. Step 2: - Calculate the middle index of the current search space to place the pivot. (not the elemenet value!) pivot = (left + right) / 2 Step 3: - Compare the middle element with the target value. If the middle element is equal to the target value, return its index. If the middle element is greater than the target value, repeat the process on the left half of the sequence. If the middle element is less than the target value, repeat the process on the right half of the sequence. NOTE: UPDATE middle element all the time! if search left then update left to mid - 1, if search right then update right to mid + 1 Step 4: - Continue dividing the search space in half until the target value is found or the search space is empty.","title":"Steps"},{"location":"algorithms/searching/binary-search/binary-search/#calculating-values","text":"Middle index is calculated using the formula: mid = left + (right - left) / 2 , where left and right represent the indices of the current search space.","title":"Calculating values"},{"location":"algorithms/searching/binary-search/binary-search/#pseudocode","text":"","title":"Pseudocode"},{"location":"algorithms/searching/binary-search/binary-search/#asymptotic-complexity","text":"Time complexity: best case: O(1) average case: O(log N) worst case: O(log N) O(log n) worst-case where n is the size of the sorted sequence. Space complexity: O(1) worst-case Only requires a constant amount of additional space for storing values.","title":"Asymptotic complexity:"},{"location":"algorithms/searching/linear-search/linear-search/","text":"LINEAR SEARCH - Iterate through a collection one element at a time slow for large datasets Does not need to be sorted useful for data structures that do not have random access (Linked lists) Steps Calculating values Pseudocode Asymptotic complexity: time complexity: O(n)","title":"LINEAR SEARCH - Iterate through a collection one element at a time"},{"location":"algorithms/searching/linear-search/linear-search/#linear-search-iterate-through-a-collection-one-element-at-a-time","text":"slow for large datasets Does not need to be sorted useful for data structures that do not have random access (Linked lists)","title":"LINEAR SEARCH - Iterate through a collection one element at a time"},{"location":"algorithms/searching/linear-search/linear-search/#steps","text":"","title":"Steps"},{"location":"algorithms/searching/linear-search/linear-search/#calculating-values","text":"","title":"Calculating values"},{"location":"algorithms/searching/linear-search/linear-search/#pseudocode","text":"","title":"Pseudocode"},{"location":"algorithms/searching/linear-search/linear-search/#asymptotic-complexity","text":"time complexity: O(n)","title":"Asymptotic complexity:"},{"location":"algorithms/shortest-path/bellman-ford/bellman-ford/","text":"","title":"Bellman ford"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/","text":"Dijktra's algorithm - Find the shortest path Examples given in a weighted graph Breadth-first search (BFS) is more commonly used for unweighted graphs Goal: To find the shortest path from a source vertex to all other vertices in a graph Steps Mark all nodes as unvisited Assign all nodes a tentative distance value Starting node has the shortest path of 0 Assign a tentative distance of infinity to all other nodes, For the current node calculate the distance to all unvisited neighbors For the current node, consider all its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value for each neighbor, and update it if the new value is less. Mark current node as visited Add the current node to the set of visited nodes. Remove it from the unvisited set. Choose new current node from unvisited nodes with minimal distance Repeat steps 3 to 5 until all nodes have been visited or the smallest tentative distance among the unvisited nodes is infinity (which means they are not reachable from the starting node). Calculating values When calculating the properties of vertices (nodes) in a graph, you should account for the following: d[v]: Represents the shortest known distance from the start node to a vertex v. This is what is set as infinity for all vertices except for the starting node, which is set as 0. \u03c0[v]: Represents the predecessor of a vertex V on the shortest path from the start node to V. Pseudocode Implementation Time Complexity \u0398((|V|+|E|)log|E|) Space Complexity","title":"Dijktra's algorithm - Find the shortest path"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/#dijktras-algorithm-find-the-shortest-path","text":"Examples given in a weighted graph Breadth-first search (BFS) is more commonly used for unweighted graphs Goal: To find the shortest path from a source vertex to all other vertices in a graph","title":"Dijktra's algorithm - Find the shortest path"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/#steps","text":"Mark all nodes as unvisited Assign all nodes a tentative distance value Starting node has the shortest path of 0 Assign a tentative distance of infinity to all other nodes, For the current node calculate the distance to all unvisited neighbors For the current node, consider all its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value for each neighbor, and update it if the new value is less. Mark current node as visited Add the current node to the set of visited nodes. Remove it from the unvisited set. Choose new current node from unvisited nodes with minimal distance Repeat steps 3 to 5 until all nodes have been visited or the smallest tentative distance among the unvisited nodes is infinity (which means they are not reachable from the starting node).","title":"Steps"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/#calculating-values","text":"When calculating the properties of vertices (nodes) in a graph, you should account for the following: d[v]: Represents the shortest known distance from the start node to a vertex v. This is what is set as infinity for all vertices except for the starting node, which is set as 0. \u03c0[v]: Represents the predecessor of a vertex V on the shortest path from the start node to V.","title":"Calculating values"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/#pseudocode","text":"","title":"Pseudocode"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/#implementation","text":"","title":"Implementation"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/#time-complexity","text":"\u0398((|V|+|E|)log|E|)","title":"Time Complexity"},{"location":"algorithms/shortest-path/dijkstras/dijkstras/#space-complexity","text":"","title":"Space Complexity"},{"location":"algorithms/sorting/bubble-sort/bubble-sort/","text":"","title":"Bubble sort"},{"location":"algorithms/sorting/insertion-sort/insertion-sort/","text":"INSERTION SORT - Sorting through insertion Sorts elements by repeatedly taking the next element and inserting it into its correct position Will form both sorted and unsorted partitions Comparisons are done from RIGHT to LEFT! Best used for a small number of records Insertion sort is a stable algorithm If binary search is used, the worst-case complexity likely remains at O(N^2) Steps Work left to right, starting in index 0 Begin with the first element in the unsorted partition (index 1) Compare current element with sorted partition from right to left . Insert the current element into its correct position in the sorted partition. Continue to the next element to the right and repeat process. NOTE: Visually, the pointer moves sequentally as comparisons and swaps are performed (from left to right). Pseudocode for i : 1 to length(A) -1 j = i while j > 0 and A[j-1] > A[j] swap A[j] and A[j-1] j = j - 1 Asymptotic complexity: Time complexity: Best case: Comparisons: O(n) Swaps: O(n) Worst case: Comparisons: O(n^2) Swaps: O(n^2) Space complexity:","title":"Insertion Sort"},{"location":"algorithms/sorting/insertion-sort/insertion-sort/#insertion-sort-sorting-through-insertion","text":"Sorts elements by repeatedly taking the next element and inserting it into its correct position Will form both sorted and unsorted partitions Comparisons are done from RIGHT to LEFT! Best used for a small number of records Insertion sort is a stable algorithm If binary search is used, the worst-case complexity likely remains at O(N^2)","title":"INSERTION SORT - Sorting through insertion"},{"location":"algorithms/sorting/insertion-sort/insertion-sort/#steps","text":"Work left to right, starting in index 0 Begin with the first element in the unsorted partition (index 1) Compare current element with sorted partition from right to left . Insert the current element into its correct position in the sorted partition. Continue to the next element to the right and repeat process. NOTE: Visually, the pointer moves sequentally as comparisons and swaps are performed (from left to right).","title":"Steps"},{"location":"algorithms/sorting/insertion-sort/insertion-sort/#pseudocode","text":"for i : 1 to length(A) -1 j = i while j > 0 and A[j-1] > A[j] swap A[j] and A[j-1] j = j - 1","title":"Pseudocode"},{"location":"algorithms/sorting/insertion-sort/insertion-sort/#asymptotic-complexity","text":"Time complexity: Best case: Comparisons: O(n) Swaps: O(n) Worst case: Comparisons: O(n^2) Swaps: O(n^2) Space complexity:","title":"Asymptotic complexity:"},{"location":"algorithms/sorting/merge-sort/merge-sort/","text":"","title":"Merge sort"},{"location":"algorithms/sorting/quick-sort/quick-sort/","text":"QUICKSORT - sorting algorithm in ascending or descending order quick sort is recursive Stop sorting when: index of itemFromLeft > index of itemFromRight itemFromLeft = is greater than pivot itemFromRight = is smaller than pivot Steps 1) Select pivot (either in left, right or midpoint) 2) Move pivot to start of array 3) Select left and right pointers 4) Partition subarray - Swapp when: a. left points to element greater than pivot b. right points to element less than pivot - Repeat until pointers cross Conditions: 1. Correct position in final sorted array 2. items to the left are smaller 3. items to the right are larger Find the pivot Partition Pseudocode Implementation Time Complexity worst-case: O(n^2) average-case: O(n log n) Space Complexity","title":"QUICKSORT - sorting algorithm in ascending or descending order"},{"location":"algorithms/sorting/quick-sort/quick-sort/#quicksort-sorting-algorithm-in-ascending-or-descending-order","text":"quick sort is recursive Stop sorting when: index of itemFromLeft > index of itemFromRight itemFromLeft = is greater than pivot itemFromRight = is smaller than pivot","title":"QUICKSORT - sorting algorithm in ascending or descending order"},{"location":"algorithms/sorting/quick-sort/quick-sort/#steps","text":"1) Select pivot (either in left, right or midpoint) 2) Move pivot to start of array 3) Select left and right pointers 4) Partition subarray - Swapp when: a. left points to element greater than pivot b. right points to element less than pivot - Repeat until pointers cross Conditions: 1. Correct position in final sorted array 2. items to the left are smaller 3. items to the right are larger","title":"Steps"},{"location":"algorithms/sorting/quick-sort/quick-sort/#find-the-pivot","text":"","title":"Find the pivot"},{"location":"algorithms/sorting/quick-sort/quick-sort/#partition","text":"","title":"Partition"},{"location":"algorithms/sorting/quick-sort/quick-sort/#pseudocode","text":"","title":"Pseudocode"},{"location":"algorithms/sorting/quick-sort/quick-sort/#implementation","text":"","title":"Implementation"},{"location":"algorithms/sorting/quick-sort/quick-sort/#time-complexity","text":"worst-case: O(n^2) average-case: O(n log n)","title":"Time Complexity"},{"location":"algorithms/sorting/quick-sort/quick-sort/#space-complexity","text":"","title":"Space Complexity"},{"location":"algorithms/sorting/randomised-quick-sort/RQS/","text":"","title":"RQS"},{"location":"algorithms/sorting/selection-sort/selection-sort/","text":"SELECTION SORT - Sorting by selecting Selection sort first finds the largest key in an unsorted list and then the next largest an so on. Few record swaps Swaps required is n\u22121 Selection sort is essentially a bubble sort except the next largest value is remembered to delay the swap to the end of each pass Selection sort is a good choice to use on an array when the swap cost is large (long strings) Overall complexity of \u0398(n^2) for worst, best and average case. Number of swaps amounts to \u0398(n) time. Steps NOTE: Pseudocode for (j = 0; j < n-1; j++) int iMin = j; for (i = j+1; i<n; i++) if (a[i] < a[iMin]) iMin = i; if (iMin != j) swap(a[j], a[iMin]); Asymptotic complexity: Time complexity: Best case: Comparisons: \u0398(n^2) Swaps: \u0398(n) Worst case: Comparisons: \u0398(n^2) Swaps: \u0398(n) Space complexity:","title":"Selection Sort"},{"location":"algorithms/sorting/selection-sort/selection-sort/#selection-sort-sorting-by-selecting","text":"Selection sort first finds the largest key in an unsorted list and then the next largest an so on. Few record swaps Swaps required is n\u22121 Selection sort is essentially a bubble sort except the next largest value is remembered to delay the swap to the end of each pass Selection sort is a good choice to use on an array when the swap cost is large (long strings) Overall complexity of \u0398(n^2) for worst, best and average case. Number of swaps amounts to \u0398(n) time.","title":"SELECTION SORT - Sorting by selecting"},{"location":"algorithms/sorting/selection-sort/selection-sort/#steps","text":"NOTE:","title":"Steps"},{"location":"algorithms/sorting/selection-sort/selection-sort/#pseudocode","text":"for (j = 0; j < n-1; j++) int iMin = j; for (i = j+1; i<n; i++) if (a[i] < a[iMin]) iMin = i; if (iMin != j) swap(a[j], a[iMin]);","title":"Pseudocode"},{"location":"algorithms/sorting/selection-sort/selection-sort/#asymptotic-complexity","text":"Time complexity: Best case: Comparisons: \u0398(n^2) Swaps: \u0398(n) Worst case: Comparisons: \u0398(n^2) Swaps: \u0398(n) Space complexity:","title":"Asymptotic complexity:"},{"location":"data-structures/graphs/graphs/","text":"GRAPHS vertices= nodes edges= undirected/directed connetion between nodes (vertices) UNDIRECTED GRAPHS Operation Adding a vertex Time Complexity: O(1) Adding an edge Time Complexity: O(1) Removing a vertex Time Complexity: O(V + E), where V is the number of vertices and E is the number of edges. Removing an edge Time Complexity: O(E) DIRECTED GRAPHS Operation WEIGHTED GRAPHS Operation UNWEIGHTED GRAPHS Operation Calculating diameter/ radius in a graph DIAMETER The maximum of all maximum distances betwen a vertex (node) to all vertices (nodes) is the diameter of a graph RADIUS The minimum of all maximum distances betwen a vertex (node) to all vertices (nodes) is the radius of a graph","title":"Graphs"},{"location":"data-structures/graphs/graphs/#graphs","text":"vertices= nodes edges= undirected/directed connetion between nodes (vertices)","title":"GRAPHS"},{"location":"data-structures/graphs/graphs/#undirected-graphs","text":"","title":"UNDIRECTED GRAPHS"},{"location":"data-structures/graphs/graphs/#operation","text":"Adding a vertex Time Complexity: O(1) Adding an edge Time Complexity: O(1) Removing a vertex Time Complexity: O(V + E), where V is the number of vertices and E is the number of edges. Removing an edge Time Complexity: O(E)","title":"Operation"},{"location":"data-structures/graphs/graphs/#directed-graphs","text":"","title":"DIRECTED GRAPHS"},{"location":"data-structures/graphs/graphs/#operation_1","text":"","title":"Operation"},{"location":"data-structures/graphs/graphs/#weighted-graphs","text":"","title":"WEIGHTED GRAPHS"},{"location":"data-structures/graphs/graphs/#operation_2","text":"","title":"Operation"},{"location":"data-structures/graphs/graphs/#unweighted-graphs","text":"","title":"UNWEIGHTED GRAPHS"},{"location":"data-structures/graphs/graphs/#operation_3","text":"","title":"Operation"},{"location":"data-structures/graphs/graphs/#calculating-diameter-radius-in-a-graph","text":"","title":"Calculating diameter/ radius in a graph"},{"location":"data-structures/graphs/graphs/#diameter","text":"The maximum of all maximum distances betwen a vertex (node) to all vertices (nodes) is the diameter of a graph","title":"DIAMETER"},{"location":"data-structures/graphs/graphs/#radius","text":"The minimum of all maximum distances betwen a vertex (node) to all vertices (nodes) is the radius of a graph","title":"RADIUS"},{"location":"data-structures/hash-tables/hashing/","text":"HASHING - Method for storing and retrieving records from a database Essentially arrays at their core stores key-value pairs Uses a hash function to map keys to indices A position in a hash table is known as a slot In Hashing, records are usually stored in an array called Hash table.","title":"Hashing"},{"location":"data-structures/hash-tables/hashing/#hashing-method-for-storing-and-retrieving-records-from-a-database","text":"Essentially arrays at their core stores key-value pairs Uses a hash function to map keys to indices A position in a hash table is known as a slot In Hashing, records are usually stored in an array called Hash table.","title":"HASHING - Method for storing and retrieving records from a database"},{"location":"data-structures/hash-tables/linear-probing/","text":"LINEAR PROBING - Probing method for resolving collisions on hash tables For to solve collisions in a table (when 2 keys have the same index value) Mainly for open addressing hash tables STEPS Compute Index: Use the hash function to compute the index for the given key. Inspect Slot: If the slot is empty, insert the key-value pair. If the slot is occupied by the same key, update the value. If the slot is occupied by a different key (collision), continue to step 3. Check Next Slot: Check the next slot in the array. If it's empty or has the same key, insert/update the key. If the end of the array is reached, wrap around to the beginning. If no empty slot is found after checking all slots, the table is full, and insertion fails. OPERATIONS Handle wrap around If the end of the array is reached, continue from the beginning.","title":"Linear Probing"},{"location":"data-structures/hash-tables/linear-probing/#linear-probing-probing-method-for-resolving-collisions-on-hash-tables","text":"For to solve collisions in a table (when 2 keys have the same index value) Mainly for open addressing hash tables","title":"LINEAR PROBING - Probing method for resolving collisions on hash tables"},{"location":"data-structures/hash-tables/linear-probing/#steps","text":"Compute Index: Use the hash function to compute the index for the given key. Inspect Slot: If the slot is empty, insert the key-value pair. If the slot is occupied by the same key, update the value. If the slot is occupied by a different key (collision), continue to step 3. Check Next Slot: Check the next slot in the array. If it's empty or has the same key, insert/update the key. If the end of the array is reached, wrap around to the beginning. If no empty slot is found after checking all slots, the table is full, and insertion fails.","title":"STEPS"},{"location":"data-structures/hash-tables/linear-probing/#operations","text":"Handle wrap around If the end of the array is reached, continue from the beginning.","title":"OPERATIONS"},{"location":"data-structures/hash-tables/separate-chaining/","text":"SEPARATE CHAING (open hashing) - HASH TABLES Used to deal with collisions in hash tables. Usually includes linked lists as an \"auxiliary\" data structure. Other data structures can be used instead. Structure: - Each slot in the hash table can be the head of a linked list - Records within a slot can be ordered in several ways: insertion order, key value order, frequency-of-access order Load factor: \u03bb = n / m - where 'n'is the number of keys stored - 'm' is total number of slots - a higher factor means the table is more full Hash colision: 2 keys hash to the same value Operations DELETION time complexity: - Average Case: O(1 + \u03bb) - Worst Case: O(n) INSERTION 1) pass element's key to hash function 2) modulus operation: n / index and then taking the reminder EXAMPLE a. 334 / 10 = 33,4 b. 334 - 10 x 33 = 4 (always remove decimal values) 3) if a value is already placed on the index, add new element to the linked list at that index time complexity: - Average Case: O(1) - Worst Case: O(1) LOOKUPS time complexity: - Average Case: O(1 + \u03bb) - Worst Case: O(n) Costs","title":"SEPARATE CHAING (open hashing) - HASH TABLES"},{"location":"data-structures/hash-tables/separate-chaining/#separate-chaing-open-hashing-hash-tables","text":"Used to deal with collisions in hash tables. Usually includes linked lists as an \"auxiliary\" data structure. Other data structures can be used instead. Structure: - Each slot in the hash table can be the head of a linked list - Records within a slot can be ordered in several ways: insertion order, key value order, frequency-of-access order Load factor: \u03bb = n / m - where 'n'is the number of keys stored - 'm' is total number of slots - a higher factor means the table is more full Hash colision: 2 keys hash to the same value","title":"SEPARATE CHAING (open hashing) - HASH TABLES"},{"location":"data-structures/hash-tables/separate-chaining/#operations","text":"","title":"Operations"},{"location":"data-structures/hash-tables/separate-chaining/#deletion","text":"time complexity: - Average Case: O(1 + \u03bb) - Worst Case: O(n)","title":"DELETION"},{"location":"data-structures/hash-tables/separate-chaining/#insertion","text":"1) pass element's key to hash function 2) modulus operation: n / index and then taking the reminder EXAMPLE a. 334 / 10 = 33,4 b. 334 - 10 x 33 = 4 (always remove decimal values) 3) if a value is already placed on the index, add new element to the linked list at that index time complexity: - Average Case: O(1) - Worst Case: O(1)","title":"INSERTION"},{"location":"data-structures/hash-tables/separate-chaining/#lookups","text":"time complexity: - Average Case: O(1 + \u03bb) - Worst Case: O(n)","title":"LOOKUPS"},{"location":"data-structures/hash-tables/separate-chaining/#costs","text":"","title":"Costs"},{"location":"data-structures/hash-tables/separate-chaining/#_1","text":"","title":""},{"location":"data-structures/lists/arrays/dynamic-arrays/","text":"DYNAMIC ARRAYS An array with a resizable capacity Random access of elements O(1) Easy to delete/insert at the end Shifting elements in O(N) Expanding/shrinking the array is O(N) STEPS Specify the initial capacity Insertion: Insert elements at the end If the array is full, double the capacity Deletion: Remove elements from the end. If the number of elements is smaller than capacity, shrink capacity (optional) Accessing: Access elements by their index (constant time) Searching: Search for elements using linear or binary search OPERATIONS ACCESSING constant time: O(1) INSERTION linear time: O(N) DELETION linear time: O(N) SEARCHING Depends on the type of search that is performed. - Linear Search : O(N) - Binary Search (if sorted) : O(log N)","title":"DYNAMIC ARRAYS"},{"location":"data-structures/lists/arrays/dynamic-arrays/#dynamic-arrays","text":"An array with a resizable capacity Random access of elements O(1) Easy to delete/insert at the end Shifting elements in O(N) Expanding/shrinking the array is O(N)","title":"DYNAMIC ARRAYS"},{"location":"data-structures/lists/arrays/dynamic-arrays/#steps","text":"Specify the initial capacity Insertion: Insert elements at the end If the array is full, double the capacity Deletion: Remove elements from the end. If the number of elements is smaller than capacity, shrink capacity (optional) Accessing: Access elements by their index (constant time) Searching: Search for elements using linear or binary search","title":"STEPS"},{"location":"data-structures/lists/arrays/dynamic-arrays/#operations","text":"","title":"OPERATIONS"},{"location":"data-structures/lists/arrays/dynamic-arrays/#accessing","text":"constant time: O(1)","title":"ACCESSING"},{"location":"data-structures/lists/arrays/dynamic-arrays/#insertion","text":"linear time: O(N)","title":"INSERTION"},{"location":"data-structures/lists/arrays/dynamic-arrays/#deletion","text":"linear time: O(N)","title":"DELETION"},{"location":"data-structures/lists/arrays/dynamic-arrays/#searching","text":"Depends on the type of search that is performed. - Linear Search : O(N) - Binary Search (if sorted) : O(log N)","title":"SEARCHING"},{"location":"data-structures/lists/arrays/static-arrays/","text":"STATIC ARRAYS Fixed in size Can't have mixed types Provides random access to elements Suitable for applications where fast element retrieval is required Space complexity: O(n) STEPS 1. OPERATIONS ACCESSING Example: array[i] gives you the element at index 'i'. Time complexity: - Accessing an element by index: O(1) INSERTION Inserting at the beginning or middle requires shifting elements to the right. Time complexity: - Insertion at the end: O(1) - Insertion at the beginning or middle: O(n) DELETION Time complexity: - Deletion at the end: O(1) - Deletion at the beginning or middle: O(n) SEARCHING BINARY SEARCH If the array is sorted, use binary search. Works by repeatedly dividing the search interval. Time complexity: - Searching for an element: O(log n) LINEAR SEARCH If array is unsorted, use linear search. Check each element one by one until finding target element. Time complexity: - Searching for an element: O(n)","title":"STATIC ARRAYS"},{"location":"data-structures/lists/arrays/static-arrays/#static-arrays","text":"Fixed in size Can't have mixed types Provides random access to elements Suitable for applications where fast element retrieval is required Space complexity: O(n)","title":"STATIC ARRAYS"},{"location":"data-structures/lists/arrays/static-arrays/#steps","text":"1.","title":"STEPS"},{"location":"data-structures/lists/arrays/static-arrays/#operations","text":"","title":"OPERATIONS"},{"location":"data-structures/lists/arrays/static-arrays/#accessing","text":"Example: array[i] gives you the element at index 'i'. Time complexity: - Accessing an element by index: O(1)","title":"ACCESSING"},{"location":"data-structures/lists/arrays/static-arrays/#insertion","text":"Inserting at the beginning or middle requires shifting elements to the right. Time complexity: - Insertion at the end: O(1) - Insertion at the beginning or middle: O(n)","title":"INSERTION"},{"location":"data-structures/lists/arrays/static-arrays/#deletion","text":"Time complexity: - Deletion at the end: O(1) - Deletion at the beginning or middle: O(n)","title":"DELETION"},{"location":"data-structures/lists/arrays/static-arrays/#searching","text":"BINARY SEARCH If the array is sorted, use binary search. Works by repeatedly dividing the search interval. Time complexity: - Searching for an element: O(log n) LINEAR SEARCH If array is unsorted, use linear search. Check each element one by one until finding target element. Time complexity: - Searching for an element: O(n)","title":"SEARCHING"},{"location":"data-structures/lists/linked-lists/linked-lists/","text":"","title":"Linked lists"},{"location":"data-structures/lists/queues/queues/","text":"","title":"Queues"},{"location":"data-structures/lists/sorted-lists/sorted-lists/","text":"","title":"Sorted lists"},{"location":"data-structures/priority-queues/binary-heaps/","text":"NOTE: - Heaps must always maintain the shape of a complete binary tree. - Each call to add will take \u0398(log n) time in the worst case. - The height of a heap with n nodes is \u2308logn + 1\u2309. - Commonly used in Priority Queues and Heap sort. (Min-heaps for PQ & Max-heaps for Heap sort) - The root node of a heap is typically at index 1. Height of heap: O(log N) Min/Max Heapify: Used during deletion, operation to restore the heap property after replacing the root with the last element. Operation starts at the root and compares current node with its children, swapping with the smaller/larger child if necessary, and continues this process down the heap. Bubble-up: Used during insertion. compares the new element with its parent and swaps if necessary, moving up the heap until the property is restored. Calculate positions: - Get Node's left child: [left(i) = 2 i] - Get node's right child: [right(i) = 2 i + 1] - Get node's parent: [floor (i/2)] Minimum-heap the value of each node is smaller than or equal to the values of its children. the minimum element is always at the root of the heap. worst-case time complexity for insertion and deletion: O(log n) where n is the number of elements in the heap. Operations: Deletion To delete the minimum element from a minimum-heap, extracting the minimum , we follow these steps: 1. Replace the root node with the last node in the heap. 2. Compare the new root with its children and swap it with the smaller child if necessary. 3. Repeat step 2 until the heap property is restored. (Min-Heapify operation) The cost of removing the minimum element is \u0398(logn) in the average and worst cases. Insertion To insert a new element into a minimum-heap, we follow these steps: 1. Add the new element as the last node in the heap. 2. Compare the new element with its parent and swap them if necessary to maintain the heap property. 3. Repeat step 2 until the heap property is restored. Continue the bubble-up process up the heap. Build-Heap To build a minimum-heap from an unordered array, we follow these steps: 1. Start from the last non-leaf node and apply the heapify operation to each node up to the root. 2. This ensures that all subtrees satisfy the heap property, resulting in a complete heap. Maximum-heap The value of each node is greater than or equal to the values of its children. the maximum element is always at the root of the heap. worst-case time complexity for insertion and deletion: O(log n) where n is the number of elements in the heap. Operations: Deletion To delete the maximum element from a maximum-heap, extracting the maximum , we follow similar steps as in the deletion operation for a minimum-heap. However, instead of swapping with the smaller child (at the root), we swap with the larger child. Complete the operation with Max-heapify. Replace the root node with the last node in the heap. Compare the new root with its children and swap it with the larger child if necessary. Repeat step 2 until the heap property is restored. (Max-Heapify operation) The cost of removing the maximum element is \u0398(logn) in the average and worst cases. Insertion To insert a new element into a maximum-heap, we follow similar steps as in the insertion operation for a minimum-heap. However, instead of comparing with the parent and swapping if necessary, we compare with the parent and swap if the new element is greater than the parent. Continue the bubble-up process up the heap. Build-Heap To build a maximum-heap from an unordered array, we follow these steps: 1. Start from the last non-leaf node and apply the heapify operation to each node up to the root. 2. This ensures that all subtrees satisfy the heap property, resulting in a complete heap. Note: The build-heap operation processes nodes in reverse level order, starting from the bottom-most level to the top-most level, and from right to left within each level. The cost of build heap is the sum of the costs for the calls to siftDown. Each siftDown operation can cost at most the number of levels it takes for the node being sifted to reach the bottom of the tree.","title":"Priority Queues"},{"location":"data-structures/priority-queues/binary-heaps/#minimum-heap","text":"the value of each node is smaller than or equal to the values of its children. the minimum element is always at the root of the heap. worst-case time complexity for insertion and deletion: O(log n) where n is the number of elements in the heap.","title":"Minimum-heap"},{"location":"data-structures/priority-queues/binary-heaps/#operations","text":"","title":"Operations:"},{"location":"data-structures/priority-queues/binary-heaps/#deletion","text":"To delete the minimum element from a minimum-heap, extracting the minimum , we follow these steps: 1. Replace the root node with the last node in the heap. 2. Compare the new root with its children and swap it with the smaller child if necessary. 3. Repeat step 2 until the heap property is restored. (Min-Heapify operation) The cost of removing the minimum element is \u0398(logn) in the average and worst cases.","title":"Deletion"},{"location":"data-structures/priority-queues/binary-heaps/#insertion","text":"To insert a new element into a minimum-heap, we follow these steps: 1. Add the new element as the last node in the heap. 2. Compare the new element with its parent and swap them if necessary to maintain the heap property. 3. Repeat step 2 until the heap property is restored. Continue the bubble-up process up the heap.","title":"Insertion"},{"location":"data-structures/priority-queues/binary-heaps/#build-heap","text":"To build a minimum-heap from an unordered array, we follow these steps: 1. Start from the last non-leaf node and apply the heapify operation to each node up to the root. 2. This ensures that all subtrees satisfy the heap property, resulting in a complete heap.","title":"Build-Heap"},{"location":"data-structures/priority-queues/binary-heaps/#maximum-heap","text":"The value of each node is greater than or equal to the values of its children. the maximum element is always at the root of the heap. worst-case time complexity for insertion and deletion: O(log n) where n is the number of elements in the heap.","title":"Maximum-heap"},{"location":"data-structures/priority-queues/binary-heaps/#operations_1","text":"","title":"Operations:"},{"location":"data-structures/priority-queues/binary-heaps/#deletion_1","text":"To delete the maximum element from a maximum-heap, extracting the maximum , we follow similar steps as in the deletion operation for a minimum-heap. However, instead of swapping with the smaller child (at the root), we swap with the larger child. Complete the operation with Max-heapify. Replace the root node with the last node in the heap. Compare the new root with its children and swap it with the larger child if necessary. Repeat step 2 until the heap property is restored. (Max-Heapify operation) The cost of removing the maximum element is \u0398(logn) in the average and worst cases.","title":"Deletion"},{"location":"data-structures/priority-queues/binary-heaps/#insertion_1","text":"To insert a new element into a maximum-heap, we follow similar steps as in the insertion operation for a minimum-heap. However, instead of comparing with the parent and swapping if necessary, we compare with the parent and swap if the new element is greater than the parent. Continue the bubble-up process up the heap.","title":"Insertion"},{"location":"data-structures/priority-queues/binary-heaps/#build-heap_1","text":"To build a maximum-heap from an unordered array, we follow these steps: 1. Start from the last non-leaf node and apply the heapify operation to each node up to the root. 2. This ensures that all subtrees satisfy the heap property, resulting in a complete heap. Note: The build-heap operation processes nodes in reverse level order, starting from the bottom-most level to the top-most level, and from right to left within each level. The cost of build heap is the sum of the costs for the calls to siftDown. Each siftDown operation can cost at most the number of levels it takes for the node being sifted to reach the bottom of the tree.","title":"Build-Heap"},{"location":"data-structures/search-trees/BST/BST/","text":"Binary Search Trees (BST) Are sorted or ordered binary trees Nodes can have 2 subtrees Items to left of given node = smaller Items to right of given node = larger Operations INSERT Start from the root. Compare the inserting node with root, if less, go left, else go right. Do step 2 until we find an empty spot in either left or right subtree. Place the inserting node at the empty spot found in step 3. Time complexity: - Average Case: O(log n) - Worst Case: O(n) SEARCH Start from the root. Compare the searching element with root, if less, go left, else go right. Do step 2 until we find the element or reach a leaf node (null). If found, return the node, else signal that the element is not in the tree. Time complexity: - Average Case: O(log n) - Worst Case: O(n) DELETE Start from the root and find the node to delete. If the node is a leaf node, we can simply remove it. If the node has one child, replace the node with its child. If the node has two children, replace the node with its in-order predecessor or in-order successor node and delete that node. You can choose to always replace with the in-order predecessor, always replace with the in-order successor, or alternate between the two. Time complexity: - Average Case: O(log n) - Worst Case: O(n) Red-Black trees (Balanced Search Trees) NIL: node at the end of a tree, usually black Specific type of balanced search trees Nodes are either red or black Root and NIL leaves are always black if a node is red, then its children are black All paths from a node to its (NIL) descendants contain the same number of black nodes The longest path (root farthest to NIL) is not larger than twice the length of the shortest path (root nearest to NIL) NOTE: BST rules still apply!!! Operations ROTATE Rotate left: Rotate right: Given a node A with left child B INSERT (Requires rotation) NOTE: same procedure as BST insertion New inserted node should be red DELETE (Requires rotation) SEARCH Time complexities Worst-case - INSERT: O(log N) - SEARCH: O(log N) - DELETE: O(log N) Amortized-case - INSERT: O(log N) - SEARCH: O(1) - DELETE: O(1) Rotations have a time complexity of O(1) Pseudocode AVL trees For any one, the height of its two subtrees differs by at most 1. Balance factor = height of left subtree - height of right subtree - Valid values of balance factor = {-1, 0, 1} Operations INSERT SEARCH DELETE Time complexities Worst-case - INSERT: O(log N) - SEARCH: O(log N) - DELETE: O(log N) Amortized-case - INSERT: \u0398(log N) - SEARCH: \u0398(log N) - DELETE: \u0398(log N) Pseudocode","title":"Binary Search Trees (BST)"},{"location":"data-structures/search-trees/BST/BST/#binary-search-trees-bst","text":"Are sorted or ordered binary trees Nodes can have 2 subtrees Items to left of given node = smaller Items to right of given node = larger","title":"Binary Search Trees (BST)"},{"location":"data-structures/search-trees/BST/BST/#operations","text":"","title":"Operations"},{"location":"data-structures/search-trees/BST/BST/#insert","text":"Start from the root. Compare the inserting node with root, if less, go left, else go right. Do step 2 until we find an empty spot in either left or right subtree. Place the inserting node at the empty spot found in step 3. Time complexity: - Average Case: O(log n) - Worst Case: O(n)","title":"INSERT"},{"location":"data-structures/search-trees/BST/BST/#search","text":"Start from the root. Compare the searching element with root, if less, go left, else go right. Do step 2 until we find the element or reach a leaf node (null). If found, return the node, else signal that the element is not in the tree. Time complexity: - Average Case: O(log n) - Worst Case: O(n)","title":"SEARCH"},{"location":"data-structures/search-trees/BST/BST/#delete","text":"Start from the root and find the node to delete. If the node is a leaf node, we can simply remove it. If the node has one child, replace the node with its child. If the node has two children, replace the node with its in-order predecessor or in-order successor node and delete that node. You can choose to always replace with the in-order predecessor, always replace with the in-order successor, or alternate between the two. Time complexity: - Average Case: O(log n) - Worst Case: O(n)","title":"DELETE"},{"location":"data-structures/search-trees/BST/BST/#red-black-trees-balanced-search-trees","text":"NIL: node at the end of a tree, usually black Specific type of balanced search trees Nodes are either red or black Root and NIL leaves are always black if a node is red, then its children are black All paths from a node to its (NIL) descendants contain the same number of black nodes The longest path (root farthest to NIL) is not larger than twice the length of the shortest path (root nearest to NIL) NOTE: BST rules still apply!!!","title":"Red-Black trees (Balanced Search Trees)"},{"location":"data-structures/search-trees/BST/BST/#operations_1","text":"","title":"Operations"},{"location":"data-structures/search-trees/BST/BST/#rotate","text":"Rotate left: Rotate right: Given a node A with left child B","title":"ROTATE"},{"location":"data-structures/search-trees/BST/BST/#insert-requires-rotation","text":"NOTE: same procedure as BST insertion New inserted node should be red","title":"INSERT (Requires rotation)"},{"location":"data-structures/search-trees/BST/BST/#delete-requires-rotation","text":"","title":"DELETE (Requires rotation)"},{"location":"data-structures/search-trees/BST/BST/#search_1","text":"","title":"SEARCH"},{"location":"data-structures/search-trees/BST/BST/#time-complexities","text":"Worst-case - INSERT: O(log N) - SEARCH: O(log N) - DELETE: O(log N) Amortized-case - INSERT: O(log N) - SEARCH: O(1) - DELETE: O(1) Rotations have a time complexity of O(1)","title":"Time complexities"},{"location":"data-structures/search-trees/BST/BST/#pseudocode","text":"","title":"Pseudocode"},{"location":"data-structures/search-trees/BST/BST/#avl-trees","text":"For any one, the height of its two subtrees differs by at most 1. Balance factor = height of left subtree - height of right subtree - Valid values of balance factor = {-1, 0, 1}","title":"AVL trees"},{"location":"data-structures/search-trees/BST/BST/#operations_2","text":"","title":"Operations"},{"location":"data-structures/search-trees/BST/BST/#insert_1","text":"","title":"INSERT"},{"location":"data-structures/search-trees/BST/BST/#search_2","text":"","title":"SEARCH"},{"location":"data-structures/search-trees/BST/BST/#delete_1","text":"","title":"DELETE"},{"location":"data-structures/search-trees/BST/BST/#time-complexities_1","text":"Worst-case - INSERT: O(log N) - SEARCH: O(log N) - DELETE: O(log N) Amortized-case - INSERT: \u0398(log N) - SEARCH: \u0398(log N) - DELETE: \u0398(log N)","title":"Time complexities"},{"location":"data-structures/search-trees/BST/BST/#pseudocode_1","text":"","title":"Pseudocode"},{"location":"data-structures/stacks/stacks/","text":"","title":"Stacks"}]}